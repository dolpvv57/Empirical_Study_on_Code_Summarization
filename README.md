# Empirical_Study_on_Code_Summarization

## Dataset

Four open-source dataset including:
### TLC

Source paper: [Summarizing Source Code with Transferred API Knowledge](https://doi.org/10.24963/ijcai.2018/314)
The dataset is available in GitHub: [TL-CodeSum/data](https://github.com/xing-hu/TL-CodeSum/tree/master/data)
We use this dataset directly, as there are no duplicates.

### Funcom
Source paper: [A neural model for generating natural language summaries of program subroutines](https://doi.org/10.1109/ICSE.2019.00087)
The dataset is available in website: [FCM](https://figshare.com/s/fe32740133b33d719ab5)
We selecte one tenth of the FCM data as the Funcom dataset for this paper. 


There are four model we reproduce in this paper: **Does Code Summarization Model Load Too Much? Function Signature May Be All That Need**

